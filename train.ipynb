{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from importlib import reload\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import models.custom.detector as g\n",
    "from libs.dataset_utils import prepare_data_from_tfrecord\n",
    "from libs.image_utils import (get_random_patch_list,\n",
    "                              draw_bounding_box,\n",
    "                              calc_iou_accuracy,\n",
    "                              calc_iou_top_1_accuracy)\n",
    "from libs.various_utils import load_from_pickle\n",
    "from configs.project_config import project_path\n",
    "\n",
    "from helpers import (evaluate,\n",
    "                     visualize,\n",
    "                     visualize_cam,\n",
    "                     restore_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constant\n",
    "\"\"\"\n",
    "NUM_CLASS = 200\n",
    "INPUT_SHAPE = (64, 64, 3)\n",
    "MODEL_BASE_INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "tfrecord_train_dir = \"{}/data/tiny_imagenet_200/tfrecord/train\".format(project_path)\n",
    "tfrecord_valid_dir = \"{}/data/tiny_imagenet_200/tfrecord/valid\".format(project_path)\n",
    "tfrecord_test_dir = \"{}/data/tiny_imagenet_200/tfrecord/test\".format(project_path)\n",
    "\n",
    "meta_path = \"{}/data/tiny_imagenet_200/meta.pickle\".format(project_path)\n",
    "pretrained_ckpt_path = \"{}/checkpoints/inception_v3/inception_v3.ckpt\".format(project_path)\n",
    "\n",
    "vanila_model_save_path = \"{}/checkpoints/vanila_inception_v3/vanila_inception_v3\".format(project_path)\n",
    "\n",
    "pickle_data_full_path = \"{}/data/tiny_imagenet_200/pickle/tiny_imagenet.pickle\".format(project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model, Dataset Pipeline and Load Pretrained Inception Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# model_base_name : { \"InceptionV3\", \"InceptionV3\", \"alexnet_v2\"}\n",
    "# 각 모델마다 final endpoint를 변경할 수 있다. final endpoint 의 정보는 models/안에 모델별 폴더안의 python 파일에 들어있다.\n",
    "model_base_name = \"InceptionV3\"\n",
    "model = g.Detector(output_dim=NUM_CLASS,\n",
    "                   input_shape=INPUT_SHAPE,\n",
    "                   model_base_input_shape=MODEL_BASE_INPUT_SHAPE,\n",
    "                   model_base_name=model_base_name,\n",
    "                   model_base_final_endpoint='Mixed_7c',\n",
    "                   model_name=\"hide_and_seek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read Data Pickle\n",
    "\"\"\"\n",
    "try :\n",
    "    d = load_from_pickle(pickle_data_full_path)\n",
    "    (X_train, P_train, Y_train, Y_train_one_hot) = (d['X_train'], d['P_train'], d['Y_train'] ,d['Y_train_one_hot'])\n",
    "    (X_valid, P_valid, Y_valid, Y_valid_one_hot) = (d['X_valid'], d['P_valid'], d['Y_valid'] ,d['Y_valid_one_hot'])\n",
    "    (X_test, P_test, Y_test, Y_test_one_hot) = (d['X_test'], d['P_test'], d['Y_test'] ,d['Y_test_one_hot'])\n",
    "    (idx_word_dict, word_idx_dict) = (d['idx_word_dict'], d['word_idx_dict'])\n",
    "    (idx_nid_dict, nid_idx_dict) = (d['idx_nid_dict'], d['nid_idx_dict'])\n",
    "except Exception as e: \n",
    "    print(\"maybe there is no pickle data: {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "with model.g.as_default():\n",
    "    \"\"\"\n",
    "    Read Data\n",
    "    \"\"\"\n",
    "    d = prepare_data_from_tfrecord(\n",
    "        tfrecord_train_dir=tfrecord_train_dir,\n",
    "        tfrecord_valid_dir=tfrecord_valid_dir,\n",
    "        tfrecord_test_dir=tfrecord_test_dir,\n",
    "        batch_size=200,\n",
    "        shuffle_buffer_size=1000)\n",
    "    (X, Y, P,\n",
    "     init_dataset_train,\n",
    "     init_dataset_train_has,\n",
    "     init_dataset_valid,\n",
    "     init_dataset_test,\n",
    "     \n",
    "     X_raw, Y_raw, P_raw,\n",
    "     init_dataset_train_raw,\n",
    "     init_dataset_valid_raw,\n",
    "     init_dataset_test_raw,) = (d['X'], d['Y'], d['P'],\n",
    "                            d['init_dataset_train'],\n",
    "                            d['init_dataset_train_has'],\n",
    "                            d['init_dataset_valid'],\n",
    "                            d['init_dataset_test'],\n",
    "                            d['X_raw'], d['Y_raw'], d['P_raw'],\n",
    "                            d['init_dataset_train_raw'],\n",
    "                            d['init_dataset_valid_raw'],\n",
    "                            d['init_dataset_test_raw'])\n",
    "\n",
    "    meta = load_from_pickle(meta_path)\n",
    "    model.meta.update(meta)\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize with pretrained weights\n",
    "    \"\"\"\n",
    "    variables_to_restore = tf.contrib.framework.get_variables_to_restore(\n",
    "        include=[model_base_name])\n",
    "    init_pretrain_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n",
    "        pretrained_ckpt_path, variables_to_restore)\n",
    "\n",
    "    init_pretrain_fn(model.sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanila Model - Learning Deep Features for Discriminative Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "\n",
    "model.train_with_dataset_api(X=X,\n",
    "                             Y=Y,\n",
    "                             init_dataset_train=init_dataset_train,\n",
    "                             init_dataset_valid=init_dataset_valid,\n",
    "                             n_epoch=2,\n",
    "                             learning_rate=0.001,\n",
    "                             reg_lambda=0.,\n",
    "                             dropout_keep_prob=0.8,\n",
    "                             patience=10,\n",
    "                             verbose_interval=1,\n",
    "                             mode=g.MODE_TRAIN_ONLY_CLF,\n",
    "                             save_dir_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_with_dataset_api(X=X,\n",
    "                             Y=Y,\n",
    "                             init_dataset_train=init_dataset_train,\n",
    "                             init_dataset_valid=init_dataset_valid,\n",
    "                             n_epoch=1,\n",
    "                             learning_rate=0.001,\n",
    "                             reg_lambda=0.,\n",
    "                             dropout_keep_prob=0.8,\n",
    "                             patience=10,\n",
    "                             verbose_interval=1,\n",
    "                             mode=g.MODE_TRAIN_GLOBAL,\n",
    "                             save_dir_path=None)\n",
    "\n",
    "model.save(vanila_model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "num_sample = 100\n",
    "X_sample =  X_train[:num_sample]\n",
    "P_sample = P_train[:num_sample]\n",
    "Y_sample_one_hot = Y_train_one_hot[:num_sample]\n",
    "Y_sample = Y_train[:num_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cam_list_sample,\n",
    "bbox_list_sample,\n",
    "gt_known_loc_accuracy_sample,\n",
    "top_1_loc_accuracy_sample) = evaluate(model, X_sample, P_sample, Y_sample_one_hot, 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cam(X_sample, Y_sample, cam_list_sample, idx_word_dict, n_show=2, start=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cam_list_train,\n",
    "bbox_list_train,\n",
    "gt_known_loc_accuracy_train,\n",
    "top_1_loc_accuracy_train) = evaluate(model, X_train, P_train, Y_train_one_hot, 'train',\n",
    "                                     flag_preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cam_list_valid,\n",
    "bbox_list_valid,\n",
    "gt_known_loc_accuracy_valid,\n",
    "top_1_loc_accuracy_valid) = evaluate(model, X_valid, P_valid, Y_valid_one_hot, 'valid',\n",
    "                                     flag_preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cam_list_test,\n",
    "bbox_list_test,\n",
    "gt_known_loc_accuracy_test,\n",
    "top_1_loc_accuracy_test) = evaluate(model, X_test, P_test, Y_test_one_hot, 'test',\n",
    "                                    flag_preprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X_train, P_train, Y_train, cam_list_train, bbox_list_train, idx_word_dict,\n",
    "          n_show=3, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X_valid, P_valid, Y_valid, cam_list_valid, bbox_list_valid, idx_word_dict,\n",
    "          n_show=3, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X_test, P_test, Y_test, cam_list_test, bbox_list_test, idx_word_dict,\n",
    "          n_show=3, start=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
